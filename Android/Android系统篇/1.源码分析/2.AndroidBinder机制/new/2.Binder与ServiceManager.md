在第一节 [1.Binder驱动的初始化]()我们讲了Binder驱动的加载和初始化，Binder驱动已经做好了准备，等待进程去开启它并使用。

servicemanager可以说是第一个使用binder驱动的进程，并且它也是作为Binder IPC的守护进程。先来看servicemanager的启动：

## servicemanager进程

### 1.init进程启动servicemanager服务

servicemanager进程由init进程通过解析init.rc文件启动的，在init事件中会启动servicemanager服务：

```c++
on init
    # ......

    # Start essential services.
    start servicemanager
```

servicemanager在文件frameworks/native/cmds/servicemanager/servicemanager.rc中声明：

```c++
service servicemanager /system/bin/servicemanager
    class core animation
    user system
    group system readproc
    critical
    onrestart restart healthd
    onrestart restart zygote
    onrestart restart audioserver
    onrestart restart media
    onrestart restart surfaceflinger
    onrestart restart inputflinger
    onrestart restart drm
    onrestart restart cameraserver
    onrestart restart keystore
    onrestart restart gatekeeperd
    onrestart restart thermalservice
    writepid /dev/cpuset/system-background/tasks
    shutdown critical
```

从定义中我们可以知道，该进程的执行程序为/system/bin/servicemanager 对应的文件为ServiceManager.cpp，它的进程名为servicemanager。

### 2.servicemanager入口

*frameworks/native/cmds/servicemanager/main.cpp*

```c++
int main(int argc, char** argv) {
    if (argc > 2) {
        LOG(FATAL) << "usage: " << argv[0] << " [binder driver]";
    }
	//如果没有参数，则默认是/dev/binder
    const char* driver = argc == 2 ? argv[1] : "/dev/binder";
	//1.获取ProcessState对象
    sp<ProcessState> ps = ProcessState::initWithDriver(driver);
    ps->setThreadPoolMaxThreadCount(0);
    ps->setCallRestriction(ProcessState::CallRestriction::FATAL_IF_NOT_ONEWAY);
	 //2.将自身作为服务添加
    sp<ServiceManager> manager = new ServiceManager(std::make_unique<Access>());
    if (!manager->addService("manager", manager, false /*allowIsolated*/, IServiceManager::DUMP_FLAG_PRIORITY_DEFAULT).isOk()) {
        LOG(ERROR) << "Could not self register servicemanager";
    }
	//3.标记成为系统服务的大管家
    IPCThreadState::self()->setTheContextObject(manager);
    ps->becomeContextManager(nullptr, nullptr);
	
    //4.开启循环，等待请求
    sp<Looper> looper = Looper::prepare(false /*allowNonCallbacks*/);

    BinderCallback::setupTo(looper);
    ClientCallbackCallback::setupTo(looper, manager);
	
    while(true) {
        looper->pollAll(-1);
    }

    // should not be reached
    return EXIT_FAILURE;
}
```

我们看到，在servicemanager.rc中，`service servicemanager /system/bin/servicemanager`后面并没有跟着参数，即传入main()中的参数是没有的，可是在main()中首先验证了参数的个数，无参的情况下，默认为/dev/binder。

那么什么时候会有参数呢？在Android8.0后，谷歌为方便设备供应商接入代码，引入Treble机制，binder机制增加了hwbinder和vndbinder，其中vndbinder的守护进程为vndservicemanager。

vndservicemanager和service共用同一份代码，只是传入的参数和宏控制的流程有部分差异。

```c++
service vndservicemanager /vendor/bin/vndservicemanager /dev/vndbinder
    class core
    user system
    group system readproc
    writepid /dev/cpuset/system-background/tasks
    shutdown critical
```

当开启的是vndbinder时，会传入参数/dev/vndbinder ，那么ServiceManager开启的驱动就是vndbinder。他们的区别如下，binder是Android Framework层与应用层之间的IPC，而vndbinder是供应商与供应商进程之间的IPC，当然还有hwbinder是Framework与供应商进程之间的IPC

![](https://img-blog.csdnimg.cn/20200329112935401.jpg)

#### 2.1 获取ProcessState对象

ServiceManager的入口首先要获取一个ProcessState对象，它是一个进程全局的单例对象，**ProcessState负责打开Binder驱动和进行内存映射**

***frameworks\native\libs\binder\ProcessState.cpp***

```c++
sp<ProcessState> ProcessState::initWithDriver(const char* driver)
{
    Mutex::Autolock _l(gProcessMutex);
    if (gProcess != nullptr) {//gProcess就是ProcessState的单例对象
        //存在gProcess直接返回
        if (!strcmp(gProcess->getDriverName().c_str(), driver)) {
            return gProcess;
        }
        LOG_ALWAYS_FATAL("ProcessState was already initialized.");
    }
	
    if (access(driver, R_OK) == -1) {
        ALOGE("Binder driver %s is unavailable. Using /dev/binder instead.", driver);
        driver = "/dev/binder";
    }
	//创建ProcessState单例对象
    gProcess = new ProcessState(driver);
    return gProcess;
}
```

当创建ProcessState对象时，在ProcessState的构造函数中，会调用open_driver打开binder驱动

***frameworks\native\libs\binder\ProcessState.cpp***

```c++
ProcessState::ProcessState(const char *driver)
    : mDriverName(String8(driver))
    , mDriverFD(open_driver(driver))//1.打开binder驱动----------------------
    , mVMStart(MAP_FAILED)
    , mThreadCountLock(PTHREAD_MUTEX_INITIALIZER)
    , mThreadCountDecrement(PTHREAD_COND_INITIALIZER)
    , mExecutingThreadsCount(0)
    , mMaxThreads(DEFAULT_MAX_BINDER_THREADS)
    , mStarvationStartTimeMs(0)
    , mBinderContextCheckFunc(nullptr)
    , mBinderContextUserData(nullptr)
    , mThreadPoolStarted(false)
    , mThreadPoolSeq(1)
    , mCallRestriction(CallRestriction::NONE)
{
    LOG_ALWAYS_FATAL("Cannot use libbinder in APEX (only system.img libbinder) since it is not stable.");
#endif

    if (mDriverFD >= 0) {
		//mmap映射内存
        //nullptr:表示映射区的起始位置由系统决定
        //BINDER_VM_SIZE: 映射区的大小
        //PROT_READ:表示映射区可读
        //MAP_PRIVATE：表示映射区的写入会有一个映射的复制 即copy-on-write 
        //mDriverFD：即要映射的文件，即Binder文件
        //0：从文件头开始映射
        mVMStart = mmap(nullptr, BINDER_VM_SIZE, PROT_READ, MAP_PRIVATE | MAP_NORESERVE, mDriverFD, 0);
        ......
    }
}
```

ProcessState会在构造时，通过函数open_driver()打开Binder驱动，并将文件描述符赋值给mDriverFD，若成功打开Binder驱动，则会通过mmap函数开启内存映射。

这里的**BINDER_VM_SIZE**代表了内存映射区域的大小，它的值为 **((1 * 1024 * 1024) - sysconf(_SC_PAGE_SIZE) * 2)**，即 1M - 8k ,  _SC_PAGE_SIZE为系统存储页的长度 4k。**这里我们可以知道，Binder一次最大可以传输 1M - 8k 的数据**。

##### 2.1.1 打开binder驱动

***frameworks\native\libs\binder\ProcessState.cpp***

```c++
static int open_driver(const char *driver)
{
    //调用Linux open函数打开binder文件 O_RDWR标志文件可读可写
    int fd = open(driver, O_RDWR | O_CLOEXEC);
    if (fd >= 0) {
        //通过ioctl函数检查binder驱动版本
        int vers = 0;
        status_t result = ioctl(fd, BINDER_VERSION, &vers);
        if (result == -1) {
            ALOGE("Binder ioctl to obtain version failed: %s", strerror(errno));
            close(fd);
            fd = -1;
        }
        if (result != 0 || vers != BINDER_CURRENT_PROTOCOL_VERSION) {
          ALOGE("Binder driver protocol(%d) does not match user space protocol(%d)! ioctl() return value: %d",
                vers, BINDER_CURRENT_PROTOCOL_VERSION, result);
            close(fd);
            fd = -1;
        }
        //通过ioctl设置binder驱动最大线程数量
        size_t maxThreads = DEFAULT_MAX_BINDER_THREADS;
        result = ioctl(fd, BINDER_SET_MAX_THREADS, &maxThreads);
        if (result == -1) {
            ALOGE("Binder ioctl to set max threads failed: %s", strerror(errno));
        }
    } else {
        ALOGW("Opening '%s' failed: %s\n", driver, strerror(errno));
    }
    return fd;
}
```

**binder驱动是以文件的形式存在的，因此这里通过Linux open函数去打开binder驱动并获得binder驱动的文件描述符并赋值给ProcessState的成员mDriverFD**。在 [1.Binder驱动的初始化 ]() 中已经介绍到open操作会调用到binder驱动程序的binder_open函数：

***/kernel/drivers/android/binder.c***

```c
//当一个进程调用函数open打开binder驱动文件后，内核会返回binder驱动的文件描述符给进程，这个文件描述符与参数file结构体filp是关联的
static int binder_open(struct inode *nodp, struct file *filp)
{
    //声明一个binder_proc结构体proc
	struct binder_proc *proc, *itr;
	struct binder_device *binder_dev;
	struct binderfs_info *info;
	struct dentry *binder_binderfs_dir_entry_proc = NULL;
	bool existing_pid = false;

	binder_debug(BINDER_DEBUG_OPEN_CLOSE, "%s: %d:%d\n", __func__,
		     current->group_leader->pid, current->pid);
	//为binder_proc结构体proc分配内存
	proc = kzalloc(sizeof(*proc), GFP_KERNEL);
	if (proc == NULL)
		return -ENOMEM;
    //初始化两个自旋锁,其中inner_lock用来保护线程以及binder_node以及所有与进程相关的的todo队列
	//outer_lock保护binder_ref
	spin_lock_init(&proc->inner_lock);
	spin_lock_init(&proc->outer_lock);
    //current是当前进程任务控制块，group_leader也就是主线程，进程的主线程表示当前进程
	get_task_struct(current->group_leader);
	//设置binder_proc成员变量tsk为当前进程主线程
	proc->tsk = current->group_leader;
    //初始化proc的todo列表
	INIT_LIST_HEAD(&proc->todo);   
	init_waitqueue_head(&proc->freeze_wait);
    //判断当前进程的调度策略是否支持，binder只支持SCHED_NORMAL,SCHED_BATCH,SCHED_FIFO,SCHED_RR。
	//prio为进程优先级，可通过normal_prio获取。一般分为实时优先级(实时进程)以及静态优先级(非实时进程)
	if (binder_supported_policy(current->policy)) {
		proc->default_priority.sched_policy = current->policy;
		proc->default_priority.prio = current->normal_prio;
	} else {
		proc->default_priority.sched_policy = SCHED_NORMAL;
		proc->default_priority.prio = NICE_TO_PRIO(0);
	}

	if (is_binderfs_device(nodp)) {
		binder_dev = nodp->i_private;
		info = nodp->i_sb->s_fs_info;
		binder_binderfs_dir_entry_proc = info->proc_log_dir;
	} else {
        //通过miscdev获取binder_device
		binder_dev = container_of(filp->private_data,
					  struct binder_device, miscdev);
	}
	refcount_inc(&binder_dev->ref);
	proc->context = &binder_dev->context;
    //初始化binder_proc的成员结构体binder_alloc
	binder_alloc_init(&proc->alloc);
    //binder驱动维护静态全局数组biner_stats，其中有一个成员数组obj_created,
    //当binder_open调用时，obj_created[BINDER_STAT_PROC]将自增。该数组用来统计binder对象的数量。
	binder_stats_created(BINDER_STAT_PROC);
    //设置binder_proc的成员变量pid为当前进程主线程的pid，即进程号
	proc->pid = current->group_leader->pid;
    //初始化delivered_death以及waiting_threads队列
	INIT_LIST_HEAD(&proc->delivered_death);
	INIT_LIST_HEAD(&proc->waiting_threads);
    //设置filp的成员变量private_data为创建的binder_proc结构体proc
	filp->private_data = proc;
	
    //判断binder_procs列表是否已经存在这个进程
	mutex_lock(&binder_procs_lock);
	hlist_for_each_entry(itr, &binder_procs, proc_node) {
		if (itr->pid == proc->pid) {
			existing_pid = true;
			break;
		}
	}
    //将proc加入到全局队列binder_procs中
	hlist_add_head(&proc->proc_node, &binder_procs);
	mutex_unlock(&binder_procs_lock);
	
    //若是进程第一次打开Bidner，
    //则在/sys/kernel/debug/binder/proc 创建进程id为名称的文件，用来记录日志
	if (binder_debugfs_dir_entry_proc && !existing_pid) {
		char strbuf[11];
		snprintf(strbuf, sizeof(strbuf), "%u", proc->pid);
		proc->debugfs_entry = debugfs_create_file(strbuf, 0444,
			binder_debugfs_dir_entry_proc,
			(void *)(unsigned long)proc->pid,
			&proc_fops);
	}
    //若是进程第一次打开Bidner，
    //则在/dev/binderfs/binder_logs/proc/proc 创建进程id为名称的文件，用来记录日志
	if (binder_binderfs_dir_entry_proc && !existing_pid) {
		char strbuf[11];
		struct dentry *binderfs_entry;

		snprintf(strbuf, sizeof(strbuf), "%u", proc->pid);
		binderfs_entry = binderfs_create_file(binder_binderfs_dir_entry_proc,
			strbuf, &proc_fops, (void *)(unsigned long)proc->pid);
		if (!IS_ERR(binderfs_entry)) {
			proc->binderfs_entry = binderfs_entry;
		} else {
			int error;

			error = PTR_ERR(binderfs_entry);
			pr_warn("Unable to create file %s in binderfs (error %d)\n",
				strbuf, error);
		}
	}

	return 0;
}

```

binder_open函数首先为进程创建一个binder_proc结构体proc，并对proc进行初始化。**binder_proc结构体用来描述一个正在使用Binder进程间通信机制的进程**。在这一过程中，会设置binder_proc结构体的成员tsk为当前进程主线程，pid为当前进程号以及进程调度策略等。binder_proc结构体proc初始化完成后会加入到一个全局hash队列binder_procs中:

`static HLIST_HEAD(binder_procs);`

binder驱动程序将所有打开了binder设备文件/dev/binder的进程都加入到全局hash队列binder_procs中，遍历整个队列即可知道当前有多少个进程在使用Binder进程通信机制。

proc最终会设置给参数file结构体filp的成员变量private_data，当一个进程调用函数open打开binder驱动文件后，内核会返回binder驱动的文件描述符给进程，这个文件描述符与参数file结构体filp是关联的。因此，当后续进程使用这个文件描述符为参数来调用binder的相应函数时，内核就会将关联的filp传递给binder驱动程序，也就可以通过private_data来获取binder_proc结构体proc

最后，若进程是第一次打开binder驱动设备并且存在相应目录，那么会在 /sys/kernel/debug/binder/proc 和 /dev/binderfs/binder_logs/proc 目录下创建以进程id为名的只读文件，通过读取该文件的内容，我们可以获得相应进程的Binder线程池、Binder实体对象、Binder引用对象以及内核缓冲区等信息。

###### 2.1.1.1 读取Binder版本、设置最大线程数量

当成功打开binder驱动设备后，通过Linux ioctl函数获取binder驱动版本以及设置binder驱动最大线程数量。通过ioctl函数可以去执行binder驱动特有的功能，内核最终会调用binder驱动程序中的binder_ioctl函数：

```c
static long binder_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
{
	int ret;
	struct binder_proc *proc = filp->private_data;
	struct binder_thread *thread;
	unsigned int size = _IOC_SIZE(cmd);
	void __user *ubuf = (void __user *)arg;

	/*pr_info("binder_ioctl: %d:%d %x %lx\n",
			proc->pid, current->pid, cmd, arg);*/

	binder_selftest_alloc(&proc->alloc);

	trace_binder_ioctl(cmd, arg);

	ret = wait_event_interruptible(binder_user_error_wait, binder_stop_on_user_error < 2);
	if (ret)
		goto err_unlocked;

	thread = binder_get_thread(proc);
	if (thread == NULL) {
		ret = -ENOMEM;
		goto err;
	}

	switch (cmd) {
	case BINDER_SET_MAX_THREADS: {
		int max_threads;

		if (copy_from_user(&max_threads, ubuf,
				   sizeof(max_threads))) {
			ret = -EINVAL;
			goto err;
		}
		binder_inner_proc_lock(proc);
		proc->max_threads = max_threads;
		binder_inner_proc_unlock(proc);
		break;
	}
	case BINDER_VERSION: {
		struct binder_version __user *ver = ubuf;

		if (size != sizeof(struct binder_version)) {
			ret = -EINVAL;
			goto err;
		}
		if (put_user(BINDER_CURRENT_PROTOCOL_VERSION,
			     &ver->protocol_version)) {
			ret = -EINVAL;
			goto err;
		}
		break;
	}
	return ret;
}
```

##### 2.1.2 进行内存映射

成功打开binder驱动之后，会进行binder内存映射

```
mVMStart = mmap(nullptr, BINDER_VM_SIZE, PROT_READ, MAP_PRIVATE | MAP_NORESERVE, mDriverFD, 0);
```





```c
static int binder_mmap(struct file *filp, struct vm_area_struct *vma)
{
	struct binder_proc *proc = filp->private_data;

	if (proc->tsk != current->group_leader)
		return -EINVAL;

	binder_debug(BINDER_DEBUG_OPEN_CLOSE,
		     "%s: %d %lx-%lx (%ld K) vma %lx pagep %lx\n",
		     __func__, proc->pid, vma->vm_start, vma->vm_end,
		     (vma->vm_end - vma->vm_start) / SZ_1K, vma->vm_flags,
		     (unsigned long)pgprot_val(vma->vm_page_prot));

	if (vma->vm_flags & FORBIDDEN_MMAP_FLAGS) {
		pr_err("%s: %d %lx-%lx %s failed %d\n", __func__,
		       proc->pid, vma->vm_start, vma->vm_end, "bad vm_flags", -EPERM);
		return -EPERM;
	}
	vma->vm_flags |= VM_DONTCOPY | VM_MIXEDMAP;
	vma->vm_flags &= ~VM_MAYWRITE;

	vma->vm_ops = &binder_vm_ops;
	vma->vm_private_data = proc;

	return binder_alloc_mmap_handler(&proc->alloc, vma);
}
```



```c
int binder_alloc_mmap_handler(struct binder_alloc *alloc,
			      struct vm_area_struct *vma)
{
	int ret;
	const char *failure_string;
	struct binder_buffer *buffer;

	mutex_lock(&binder_alloc_mmap_lock);
	if (alloc->buffer_size) {
		ret = -EBUSY;
		failure_string = "already mapped";
		goto err_already_mapped;
	}
    //比较vma中记录的地址范围和4m的大小，选择小的
	alloc->buffer_size = min_t(unsigned long, vma->vm_end - vma->vm_start,
				   SZ_4M);
	mutex_unlock(&binder_alloc_mmap_lock);

	alloc->buffer = (void __user *)vma->vm_start;

	alloc->pages = kcalloc(alloc->buffer_size / PAGE_SIZE,
			       sizeof(alloc->pages[0]),
			       GFP_KERNEL);
	if (alloc->pages == NULL) {
		ret = -ENOMEM;
		failure_string = "alloc page array";
		goto err_alloc_pages_failed;
	}

	buffer = kzalloc(sizeof(*buffer), GFP_KERNEL);
	if (!buffer) {
		ret = -ENOMEM;
		failure_string = "alloc buffer struct";
		goto err_alloc_buf_struct_failed;
	}

	buffer->user_data = alloc->buffer;
	list_add(&buffer->entry, &alloc->buffers);
	buffer->free = 1;
	binder_insert_free_buffer(alloc, buffer);
	alloc->free_async_space = alloc->buffer_size / 2;
	binder_alloc_set_vma(alloc, vma);
	mmgrab(alloc->vma_vm_mm);

	return 0;

err_alloc_buf_struct_failed:
	kfree(alloc->pages);
	alloc->pages = NULL;
err_alloc_pages_failed:
	alloc->buffer = NULL;
	mutex_lock(&binder_alloc_mmap_lock);
	alloc->buffer_size = 0;
err_already_mapped:
	mutex_unlock(&binder_alloc_mmap_lock);
	binder_alloc_debug(BINDER_DEBUG_USER_ERROR,
			   "%s: %d %lx-%lx %s failed %d\n", __func__,
			   alloc->pid, vma->vm_start, vma->vm_end,
			   failure_string, ret);
	return ret;
}

```



#### 2.2 添加Service

开启binder驱动后，main函数进行创建ServiceManager对象，并通过它自己的addService方法将自身作为一个服务加入管理中，后续Android系统的其他服务入AMS，PMS等都会通过该方法添加到ServiceManager中进行管理，这里来分析addService方法

```c++
//这里可以看到添加的服务的对象需要继承IBinder
Status ServiceManager::addService(const std::string& name, const sp<IBinder>& binder, bool allowIsolated, int32_t dumpPriority) {
    auto ctx = mAccess->getCallingContext();

    // 普通应用不能添加服务
    if (multiuser_get_app_id(ctx.uid) >= AID_APP) {
        return Status::fromExceptionCode(Status::EX_SECURITY);
    }
	//判断是否有权限添加服务
    if (!mAccess->canAdd(ctx, name)) {
        return Status::fromExceptionCode(Status::EX_SECURITY);
    }
	
    if (binder == nullptr) {
        return Status::fromExceptionCode(Status::EX_ILLEGAL_ARGUMENT);
    }

    if (!isValidServiceName(name)) {
        LOG(ERROR) << "Invalid service name: " << name;
        return Status::fromExceptionCode(Status::EX_ILLEGAL_ARGUMENT);
    }

#ifndef VENDORSERVICEMANAGER
    if (!meetsDeclarationRequirements(binder, name)) {
        // already logged
        return Status::fromExceptionCode(Status::EX_ILLEGAL_ARGUMENT);
    }
#endif

    //这里需要验证binder的远程对象不能为空，以及没有死亡
    if (binder->remoteBinder() != nullptr && binder->linkToDeath(this) != OK) {
        LOG(ERROR) << "Could not linkToDeath when adding " << name;
        return Status::fromExceptionCode(Status::EX_ILLEGAL_STATE);
    }
	//创建一个服务对象并添加到map中
    mNameToService[name] = Service {
        .binder = binder,
        .allowIsolated = allowIsolated,
        .dumpPriority = dumpPriority,
        .debugPid = ctx.debugPid,
    };

    auto it = mNameToRegistrationCallback.find(name);
    if (it != mNameToRegistrationCallback.end()) {
        for (const sp<IServiceCallback>& cb : it->second) {
            mNameToService[name].guaranteeClient = true;
            // permission checked in registerForNotifications
            cb->onRegistration(name, binder);
        }
    }
	//一路顺畅的执行到这里，就返回ok！
    return Status::ok();
}

```

可以看到addService函数比较简单，就是创建了个Service对象，并以传入的服务名称作为key，以Service作为value添加到map中

#### 2.3 成为Binder驱动的上下文管理器

前文将过ProcessState是用于管理Binder驱动，而具体与Binder驱动进行交互就要通过IPCThreadState了

*frameworks/native/cmds/servicemanager/main.cpp*

```c
IPCThreadState::self()->setTheContextObject(manager);
ps->becomeContextManager(nullptr, nullptr);
```
首先调用IPCThreadState的setTheContextObject函数设置成员the_context_object为ServiceManager对象

```c++
void IPCThreadState::setTheContextObject(sp<BBinder> obj)
{
    the_context_object = obj;
}
```

然后调用ProcessState的becomeContextManager使servicemanager成为Binder的上下文管理器

```c++
bool ProcessState::becomeContextManager(context_check_func checkFunc, void* userData)
{
    AutoMutex _l(mLock);
	//......
    flat_binder_object obj {
        .flags = FLAT_BINDER_FLAG_TXN_SECURITY_CTX,
    };

    int result = ioctl(mDriverFD, BINDER_SET_CONTEXT_MGR_EXT, &obj);
	//......

    return result == 0;
}
```

由于形参checkFunc和userData都为null，这里省略了相关的代码，becomeContextManager的核心代码就是通过ioctl调用mDriverFD是ProcessState的成员变量，是打开binder驱动时获取的binder驱动设备的文件描述符。ioctl函数通过这个文件描述符，调用Binder驱动的binder_ioctl函数，这里传入命令BINDER_SET_CONTEXT_MGR_EXT 以及一个 flat_binder_object 结构体，然后会走到binder驱动的binder_ioctl函数中：

```c
static long binder_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
{
	int ret;
    //获取filp中的binder_proc结构体private_data
	struct binder_proc *proc = filp->private_data;
	struct binder_thread *thread;
	unsigned int size = _IOC_SIZE(cmd);
    //说明参数是用户空间的指，指向用户空间的地址
	void __user *ubuf = (void __user *)arg;

	binder_selftest_alloc(&proc->alloc);

	trace_binder_ioctl(cmd, arg);

	ret = wait_event_interruptible(binder_user_error_wait, binder_stop_on_user_error < 2);
	if (ret)
		goto err_unlocked;

	thread = binder_get_thread(proc);
	if (thread == NULL) {
		ret = -ENOMEM;
		goto err;
	}

	switch (cmd) {
	case BINDER_SET_CONTEXT_MGR_EXT: {
        //创建结构体fbo后，把参数从用户空间拷贝数据赋值给fbo
		struct flat_binder_object fbo;
		if (copy_from_user(&fbo, ubuf, sizeof(fbo))) {
			ret = -EINVAL;
			goto err;
		}
        //调用binder_ioctl_set_ctx_mgr
		ret = binder_ioctl_set_ctx_mgr(filp, &fbo);
		if (ret)
			goto err;
		break;
	}
	case BINDER_SET_CONTEXT_MGR:
		ret = binder_ioctl_set_ctx_mgr(filp, NULL);
		if (ret)
			goto err;
		break;
	}
	ret = 0;
err:
	if (thread)
		thread->looper_need_return = false;
	wait_event_interruptible(binder_user_error_wait, binder_stop_on_user_error < 2);
	if (ret && ret != -EINTR)
		pr_info("%d:%d ioctl %x %lx returned %d\n", proc->pid, current->pid, cmd, arg, ret);
	return ret;
}
```





```c
static int binder_ioctl_set_ctx_mgr(struct file *filp,
				    struct flat_binder_object *fbo)
{
	int ret = 0;
    //获取filp中的binder_proc结构体private_data
	struct binder_proc *proc = filp->private_data;
    //获取binder_proc中记录的binder_context结构体，这里记录着binder驱动的上下文管理器
	struct binder_context *context = proc->context;
	struct binder_node *new_node;
	kuid_t curr_euid = current_euid();

	mutex_lock(&context->context_mgr_node_lock);
 	 //若已经设置了context manager上下文管理器，那么返回失败
	if (context->binder_context_mgr_node) {
		pr_err("BINDER_SET_CONTEXT_MGR already set\n");
		ret = -EBUSY;
		goto out;
	}
    //检查当前进程是否具有注册Context Manager的SEAndroid安全权限
	ret = security_binder_set_context_mgr(proc->tsk);
	if (ret < 0)
		goto out;
	if (uid_valid(context->binder_context_mgr_uid)) {
		if (!uid_eq(context->binder_context_mgr_uid, curr_euid)) {
			pr_err("BINDER_SET_CONTEXT_MGR bad uid %d != %d\n",
			       from_kuid(&init_user_ns, curr_euid),
			       from_kuid(&init_user_ns,
					 context->binder_context_mgr_uid));
			ret = -EPERM;
			goto out;
		}
	} else {
		context->binder_context_mgr_uid = curr_euid;
	}
    //创建一个binder_node结构体对象，表示ServiceManager
	new_node = binder_new_node(proc, fbo);
	if (!new_node) {
		ret = -ENOMEM;
		goto out;
	}
	binder_node_lock(new_node);
	new_node->local_weak_refs++;
	new_node->local_strong_refs++;
	new_node->has_strong_ref = 1;
	new_node->has_weak_ref = 1;
    //设置binder_context结构体中的binder_context_mgr_node为新创建的binder_node对象
	context->binder_context_mgr_node = new_node;
	binder_node_unlock(new_node);
	binder_put_node(new_node);
out:
	mutex_unlock(&context->context_mgr_node_lock);
	return ret;
}
```

函数binder_ioctl_set_ctx_mgr首先判断Binder进程间通信机制的上下文管理器是否已经被设置了以及是否有权限，若通过了这两个条件，则通过binder_new_node为ServiceManager创建一个Binder实体对象，并记录在context->binder_context_mgr_node中，接着程序从binder驱动内核空间返回到ServiceManager进程用户空间

#### 2.4 ServiceManager循环等待请求

```c
    //4.开启循环，等待请求
    sp<Looper> looper = Looper::prepare(false /*allowNonCallbacks*/);

    BinderCallback::setupTo(looper);
    ClientCallbackCallback::setupTo(looper, manager);
	
    while(true) {
        looper->pollAll(-1);
    }
```

首先创建native端的Looper对象，设置looper的事件回调，最后在一个无限循环中进行调用looper的pollAll函数。

